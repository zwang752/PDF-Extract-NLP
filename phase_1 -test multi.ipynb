{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# manager = multiprocessing.Manager()\n",
    "# projectlist = manager.list()\n",
    "# ttable=manager.list()\n",
    "big_table = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variable for TRANSLATOR_TEXT_KEY is not set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nreTrans(df_result)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fnx and fnc are file path for final output\n",
    "import os\n",
    "load_path = os.getcwd()\n",
    "fnx=load_path + '/output44/2018_COP_phase_1_result.xlsx'\n",
    "fnc=load_path + '/output44/2018_COP_phase_1_result.csv'\n",
    "\n",
    "\n",
    "#Import all the necessary modules and packages\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "# from pdfminer.converter import TextConverter, XMLConverter, HTMLConverter\n",
    "# from pdfminer.layout import LAParams\n",
    "# from pdfminer.pdfpage import PDFPage\n",
    "# from io import BytesIO\n",
    "# import re\n",
    "# import importlib\n",
    "# importlib.reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# from google.cloud import logging\n",
    "# from datetime import timedelta\n",
    "# from itertools import product\n",
    "# import pandas_market_calendars as mcal\n",
    "# from datetime import date,timedelta,datetime\n",
    "# from google.cloud import translate\n",
    "# from google.cloud import bigquery\n",
    "# from google.cloud.bigquery import SchemaField,client\n",
    "# from google.cloud.bigquery import client\n",
    "# from newspaper import Article\n",
    "# from newspaper import fulltext\n",
    "# import xlsxwriter\n",
    "# from textblob import TextBlob\n",
    "# from openpyxl import load_workbook\n",
    "# from gensim import corpora, models\n",
    "# from gensim.summarization import summarize\n",
    "# import gensim\n",
    "# Gensim\n",
    "# import gensim\n",
    "# import gensim.corpora as corpora\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.models import CoherenceModel\n",
    "#\n",
    "# # spacy for lemmatization\n",
    "# import spacy\n",
    "#\n",
    "# # Plotting tools\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim  # don't skip this\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from google.cloud import language\n",
    "# from google.cloud.language import enums\n",
    "# from google.cloud.language import types\n",
    "# from sumy.parsers.plaintext import PlaintextParser #We're choosing a plaintext parser here, other parsers available for HTML etc.\n",
    "# from sumy.nlp.tokenizers import Tokenizer\n",
    "# from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "# from sumy.nlp.stemmers import Stemmer\n",
    "# from sumy.utils import get_stop_words\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import collections\n",
    "# import unicodedata\n",
    "# import string\n",
    "# import argparse\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from stop_words import get_stop_words\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# import datetime as dt\n",
    "# # Enable logging for gensim - optional\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "# from openpyxl import load_workbook\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "\n",
    "import img2pdf\n",
    "from PIL import Image\n",
    "import textract\n",
    "import PyPDF2\n",
    "from langdetect import detect\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import textwrap\n",
    "import requests\n",
    "import uuid\n",
    "import glob\n",
    "#### Time out\n",
    "import signal\n",
    "from time import sleep    # only needed for testing\n",
    "import docx\n",
    "import pikepdf\n",
    "\n",
    "#generate a new function called getpagenum, the input is the directory of an article\n",
    "#the output is the number of page of each article\n",
    "# def getpagenum(path):\n",
    "#                 pdf = PyPDF2.PdfFileReader(open(path, \"rb\"),strict=False)\n",
    "#                 num_of_pages = pdf.getNumPages()\n",
    "#                 return num_of_pages\n",
    "\n",
    "# Custom exception for the timeout\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "# Handler function to be called when SIGALRM is received\n",
    "def sigalrm_handler(signum, frame):\n",
    "    # We get signal!\n",
    "    raise TimeoutException()\n",
    "\n",
    "##### Convert Docx and PPTX: PPTX returns a list\n",
    "from pptx import Presentation\n",
    "# path_to_presentation=directory+'/'+str('451161')+'/original/'+ 'Pacto_Global_-_COP_2016-17.pptx'\n",
    "def getPPT(path_to_presentation):\n",
    "    prs = Presentation(path_to_presentation)\n",
    "\n",
    "    # text_runs will be populated with a list of strings,\n",
    "    # one for each text run in presentation\n",
    "    text_runs = []\n",
    "    slide_num = 1\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_text_frame:\n",
    "                continue\n",
    "            for paragraph in shape.text_frame.paragraphs:\n",
    "                for run in paragraph.runs:\n",
    "                    text_runs.append(run.text)\n",
    "        slide_num += 1\n",
    "    return [slide_num, text_runs]\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "'''\n",
    "def get_translated_text(text):\n",
    "    body = [{'text': text}]\n",
    "    request = requests.post(constructed_url, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "    #return response\n",
    "    return response[0][\"translations\"][0][\"text\"]\n",
    "\n",
    "def reTrans(df):\n",
    "    #This is to check if we failed to translate any row in df.\n",
    "    #If there is, we try to translate them agin\n",
    "    mask = df['translation'] == 'Fail to translate'\n",
    "    def reTrans_helper(txt):\n",
    "        if len(txt)>=unit_limit:\n",
    "            #raw_str = temp_str\n",
    "            print(str(i),df_result.corp_id[i],file_name,'too big to translate')\n",
    "            return 'Too big'\n",
    "        else:\n",
    "            try:\n",
    "                if len(txt) <= 4000:\n",
    "                    return get_translated_text(txt)\n",
    "                else:\n",
    "                    lines = textwrap.wrap(txt, 4000, break_long_words=False)\n",
    "                    #translate_client = translate.Client()\n",
    "                    return ''.join([get_translated_text(x) for x in lines])\n",
    "                print(str(i),df_result.corp_id[i],file_name,'Translated')\n",
    "            except Exception as e:\n",
    "                #raw_str = temp_str\n",
    "                print(str(i),df_result.corp_id[i],file_name,'Fail to translate')\n",
    "                print(\"ERROR MESSAGE reTrans: \")\n",
    "                print(e)\n",
    "                return 'Fail to translate'\n",
    "\n",
    "    df.loc[mask, 'translation'] = df.loc[mask, 'text'].apply(reTrans_helper)\n",
    "    #df['translation'] = df['text'].apply(lambda text: get_translated_text(text) if )\n",
    "'''\n",
    "\n",
    "def pdf2text_and_numpage(filename):\n",
    "    #write a for-loop to open many files\n",
    "    #open allows you to read the file\n",
    "    pdfFileObj = open(filename,'rb')\n",
    "#The pdfReader variable is a readable object that will be parsed\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "#discerning the number of pages will allow us to parse through all #the pages\n",
    "    num_pages = pdfReader.numPages\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "#The while loop will read each page\n",
    "    while count < num_pages:\n",
    "        pageObj = pdfReader.getPage(count)\n",
    "        count +=1\n",
    "        text += pageObj.extractText()\n",
    "\n",
    "    #This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
    "    if text != \"\":\n",
    "        if '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n' in text:\n",
    "            return (str(textract.process(filename, method='tesseract')), num_pages, 'text-based-PDF')\n",
    "        else:\n",
    "            return (str(text), num_pages, 'text-based-PDF')\n",
    "    #If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "    else:\n",
    "        return (str(textract.process(filename, method='tesseract')), num_pages, 'image-based-PDF')\n",
    "    # Now we have a text variable which contains all the text derived #from our PDF file. Type print(text) to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\\n' etc.\n",
    "    # Now, we will clean our text variable, and return it as a list of keywords.\n",
    "\n",
    "def image2text_and_numpage(img_path, path2original):\n",
    "    # storing pdf path\n",
    "    pdf_path = path2original + 'converted.pdf'\n",
    "    # opening image\n",
    "    image = Image.open(img_path)\n",
    "    # converting into chunks using img2pdf\n",
    "    pdf_bytes = img2pdf.convert(image.filename)\n",
    "    # opening or creating pdf file\n",
    "    file = open(pdf_path, \"wb\")\n",
    "    # writing pdf files with chunks\n",
    "    file.write(pdf_bytes)\n",
    "    # closing image file\n",
    "    image.close()\n",
    "    # closing pdf file\n",
    "    file.close()\n",
    "    return pdf2text_and_numpage(pdf_path)\n",
    "\n",
    "def decrypt_code(filename, path2original):\n",
    "    if PyPDF2.PdfFileReader(open(filename, 'rb')).isEncrypted:\n",
    "        pdf_old = pikepdf.open(filename)\n",
    "        pdf_path = path2original+'extractable.pdf'\n",
    "        pdf_old.save(pdf_path)\n",
    "        return pdf2text_and_numpage(pdf_path)\n",
    "    # skip file? Write to a log?\n",
    "    else:\n",
    "        return pdf2text_and_numpage(filename)\n",
    "\n",
    "'''\n",
    "def translation_clean(text):\n",
    "    cidcompile1=re.compile(r'\\(*\\s*\\(*\\s*[cC]\\s*i\\s*\\)*\\s*d\\s*\\:*\\s*\\:*(?:\\(*\\s*[cC]\\s*i\\s*d\\s*\\:*)?\\:*\\s*[0-9]+\\s*[0-9]{0,}\\s*\\)*\\s*\\)*')\n",
    "    cidcompile2=re.compile(r'\\(\\s*[cC]\\s*i\\s*\\)*\\s*d\\s*\\:')\n",
    "    cidcompile3=re.compile(r'\\:\\s*[0-9]+\\s*\\)')\n",
    "    punctuation = re.compile(r',.:?!<>{};')\n",
    "    \n",
    "    t_str=re.sub(cidcompile1,' ',text)\n",
    "    t_str=re.sub(cidcompile2,' ',t_str)\n",
    "    t_str=re.sub(cidcompile3,' ',t_str)\n",
    "    t_str=re.sub(punctuation,' ',t_str)\n",
    "    t_str=re.sub(\"\\n\",' ',t_str)\n",
    "    \n",
    "    control = re.compile('\\x00|\\x01|\\x02|\\x03|\\x04|\\x05|\\x06|\\x07|\\x08|\\x09|\\x0a|\\x0b|\\x0c|\\x0d|\\x0e|\\x0f|\\x10|\\x11|\\x12|\\x13|\\x14|\\x15|\\x16|\\x17|\\x18|\\x19|\\x1a|\\x1b|\\x1c|\\x1d|\\x1e|\\x1f|\\x7f|\\xc2\\x80|\\xc2\\x81|\\xc2\\x82|\\xc2\\x83|\\xc2\\x84|\\xc2\\x85|\\xc2\\x86|\\xc2\\x87|\\xc2\\x88|\\xc2\\x89|\\xc2\\x8a|\\xc2\\x8b|\\xc2\\x8c|\\xc2\\x8d|\\xc2\\x8e|\\xc2\\x8f|\\xc2\\x90|\\xc2\\x91|\\xc2\\x92|\\xc2\\x93|\\xc2\\x94|\\xc2\\x95|\\xc2\\x96|\\xc2\\x97|\\xc2\\x98|\\xc2\\x99|\\xc2\\x9a|\\xc2\\x9b|\\xc2\\x9c|\\xc2\\x9d|\\xc2\\x9e|\\xc2\\x9f')\n",
    "    \n",
    "    t_str = re.sub(control,' ',t_str)\n",
    "    \n",
    "    return t_str\n",
    "'''\n",
    "\n",
    "## save all the folders name of articles into the corp_id list\n",
    "directory = load_path + '/COP44/'\n",
    "os.chdir(directory)\n",
    "\n",
    "\n",
    "#print(os.getcwd())\n",
    "corp_id = [x for x in glob.glob('*')]\n",
    "\n",
    "#This step is to save all the name and id of articles in list1 and files_name list\n",
    "\n",
    "#num is used to calculate the number of empty folders\n",
    "num = 0\n",
    "#a is used to calculate the number of non-empty folders\n",
    "a = 0\n",
    "#list1 is used to save all the id of non-empty articles\n",
    "list1 = []\n",
    "#files_name is a list used to save all the names of non-empty articles\n",
    "files_name = []\n",
    "\n",
    "for i in corp_id:\n",
    "    ##each path is the path of each articles\n",
    "    path = directory + '/' + i + '/original'\n",
    "    for dirpath, dirnames, files in os.walk(path):\n",
    "        if not files:\n",
    "            #print(dirpath, 'is empty')\n",
    "            num += 1\n",
    "        else:\n",
    "            a += 1\n",
    "            #print(dirpath,'is not empty')\n",
    "            list1.append(i)\n",
    "            files_name.append(files)\n",
    "\n",
    "#generate a dataframe which includes the corporate_id and size\n",
    "#d=pd.DataFrame(columns=['corpor_id','file_size_in_bytes'])\n",
    "#result_list is a list of list, which contains all id, file names and size of all the elements of articles\n",
    "result_list = []\n",
    "fail_file = []\n",
    "for i in range(0, len(files_name)):\n",
    "    if len(files_name[i]) == 1:\n",
    "        ## file_name is the first element of each item in files_name list\n",
    "        file_name = str(files_name[i][0])\n",
    "        try:\n",
    "            #### list1 is id\n",
    "            path1 = directory + '/' + str(list1[i]) + '/original/' + file_name\n",
    "            ## statinfo is the size of each file Size in bytes of a plain file; amount of data waiting on some special files.\n",
    "            statinfo = os.stat(path1).st_size\n",
    "            result_list.append([list1[i],file_name,statinfo])\n",
    "        except:\n",
    "            fail_file.append([i,list1[i],file_name])\n",
    "            pass\n",
    "    else:\n",
    "        for l in range(0,len(files_name[i])):\n",
    "            file_name = str(files_name[i][l])\n",
    "            try:\n",
    "                if \"desktop\" in file_name:\n",
    "                    continue\n",
    "                else:\n",
    "                    path1 = directory + '/' + str(list1[i]) + '/original/' + file_name\n",
    "                    statinfo = os.stat(path1).st_size\n",
    "                    result_list.append([list1[i], file_name, statinfo])\n",
    "            except:\n",
    "                fail_file.append([i,list1[i],file_name])\n",
    "                pass\n",
    "\n",
    "labels = ['corp_id', 'file_name', 'file_size_in_bytes']\n",
    "#generate a dataframe called df_result with column names 'corp_id','file_name','size'\n",
    "#the elements in df_result is result_list\n",
    "df_result = pd.DataFrame(result_list, columns=labels)\n",
    "\n",
    "####LIMIT FOR CLOUD JSON IS DIFFERENT\n",
    "unit_limit= 10000000\n",
    "#os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]='/home/globalaicloud/UNGC_NEW/JSON/My First Project-dec55926c2d0.json'\n",
    "\n",
    "# Checks to see if the Translator Text subscription key is available\n",
    "# as an environment variable. If you are setting your subscription key as a\n",
    "# string, then comment these lines out.\n",
    "if 'TRANSLATOR_TEXT_KEY' in os.environ:\n",
    "    subscriptionKey = os.environ['TRANSLATOR_TEXT_KEY']\n",
    "else:\n",
    "    print('Environment variable for TRANSLATOR_TEXT_KEY is not set.')\n",
    "    #exit()\n",
    "# If you want to set your subscription key as a string, uncomment the line\n",
    "# below and add your subscription key.\n",
    "subscriptionKey = \"331f7ace25a849639d0d319181758dff\"\n",
    "\n",
    "base_url = 'https://api.cognitive.microsofttranslator.com'\n",
    "path = '/translate?api-version=3.0'\n",
    "params = '&to=en'\n",
    "constructed_url = base_url + path + params\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': subscriptionKey,\n",
    "    'Content-type': 'application/json',\n",
    "    'X-ClientTraceId': str(uuid.uuid4())}\n",
    "\n",
    "timelimit_seconds = 60*30    # Must be an integer\n",
    "\n",
    "\n",
    "### fill the columns in each row\n",
    "###\"file format\" \"page number\" \"original text\" \"\"\n",
    "\n",
    "def extract_text(i):\n",
    "    path1 = directory + '/' + str(df_result.corp_id[i]) + '/original/' + df_result.file_name[i]\n",
    "    path2original = directory + '/' + str(df_result.corp_id[i]) + '/original/'\n",
    "    file_name = str(df_result.file_name[i])\n",
    "    if file_name.endswith('.doc') or file_name.endswith('.docx'):\n",
    "        df_result.loc[i, 'file_format'] = 'Word File'\n",
    "        df_result.loc[i, 'page_num'] = np.NaN\n",
    "        try:\n",
    "            df_result.loc[i, 'text'] = getText(path1)\n",
    "            print(str(i), df_result.corp_id[i], file_name, 'Doc converted')\n",
    "        except:\n",
    "            df_result.loc[i, 'text'] = 'Wrong Format'\n",
    "            print(str(i), df_result.corp_id[i], file_name, 'Doc unconverted')\n",
    "    elif file_name.endswith('.ppt') or file_name.endswith('.pptx'):\n",
    "        df_result.loc[i, 'file_format'] = 'Powerpoint'\n",
    "        try:\n",
    "            ppt_text = getPPT(path1)\n",
    "            df_result.loc[i, 'page_num'] = ppt_text[0]\n",
    "            df_result.loc[i, 'text'] = ''.join(x for x in ppt_text[1])\n",
    "            print(str(i), df_result.corp_id[i], file_name, 'PPT converted')\n",
    "        except:\n",
    "            df_result.loc[i, 'page_num'] = np.NaN\n",
    "            df_result.loc[i, 'text'] = 'Wrong Format'\n",
    "            print(str(i), df_result.corp_id[i], file_name, 'PPT unconverted')\n",
    "    elif file_name.endswith('.jpg') or file_name.endswith('.jpeg') or file_name.endswith('.png'):\n",
    "        try:\n",
    "            wildcard, file_extension = os.path.splitext(path1)\n",
    "            image_text, num_page, wildcard = image2text_and_numpage(path1, path2original)\n",
    "            df_result.loc[i, 'page_num'] = num_page\n",
    "            df_result.loc[i, 'text'] = image_text\n",
    "            df_result.loc[i, 'file_format'] = file_extension[1:]\n",
    "            print(str(i), df_result.corp_id[i], file_name, 'image converted')\n",
    "        #         df_result.loc[i,'file_format']='Image'\n",
    "        #         df_result.loc[i,'page_num']=np.NaN\n",
    "        #         df_result.loc[i,'text']='Wrong Format'\n",
    "        #         print(str(i),df_result.corp_id[i],file_name, 'Wrong Format')\n",
    "        except Exception as e:\n",
    "            print(\"Error Message for IMAGE READ: \")\n",
    "            print(e)\n",
    "    elif file_name.endswith('.xlsx') or file_name.endswith('.csv'):\n",
    "        df_result.loc[i, 'file_format'] = 'Excel'\n",
    "        df_result.loc[i, 'page_num'] = np.NaN\n",
    "        df_result.loc[i, 'text'] = 'Wrong Format'\n",
    "        print(str(i), df_result.corp_id[i], file_name, 'Wrong Format')\n",
    "    else:\n",
    "        # df_result.loc[i,'file_format']='pdf'\n",
    "        \n",
    "        raw_str = ''\n",
    "        \n",
    "        # Set up signal handler for SIGALRM, saving previous value\n",
    "        \n",
    "        old_handler = signal.signal(signal.SIGALRM, sigalrm_handler)\n",
    "        \n",
    "        # Start timer\n",
    "        \n",
    "        signal.alarm(timelimit_seconds)\n",
    "        \n",
    "        try:\n",
    "            # raw = parser.from_file('sample.pdf')\n",
    "            # print(raw['content'])\n",
    "            # pdf_text = tika.parser.from_file(path1)\n",
    "            pdf_text, num_page, pdf_type = decrypt_code(path1, path2original)\n",
    "            df_result.loc[i, 'page_num'] = num_page\n",
    "            df_result.loc[i, 'text'] = pdf_text\n",
    "            df_result.loc[i, 'file_format'] = pdf_type\n",
    "            print(str(i), df_result.corp_id[i], file_name, 'PDF converted')\n",
    "            # print('PDF TEXT:')\n",
    "            # print(pdf_text[1])\n",
    "        except Exception as e:\n",
    "            if str(e) is \"\":\n",
    "                df_result.loc[i, 'text'] = 'Timeout'\n",
    "                df_result.loc[i, 'language'] = 'Timeout'\n",
    "                df_result.loc[i, 'page_num'] = np.NaN  \n",
    "                print(str(i), df_result.corp_id[i], file_name, 'took too long to convert')\n",
    "            else:\n",
    "                print('ERROR MESSAGE PDF:')\n",
    "                print(e)\n",
    "                df_result.loc[i, 'text'] = 'Convert Manually'\n",
    "                df_result.loc[i, 'page_num'] = np.NaN\n",
    "                df_result.loc[i, 'language'] = 'Error'\n",
    "                print(str(i), df_result.corp_id[i], file_name, 'fail to convert')\n",
    "        finally:\n",
    "            # Turn off timer\n",
    "            signal.alarm(0)\n",
    "            # Restore handler to previous value\n",
    "            signal.signal(signal.SIGALRM, old_handler)\n",
    "\n",
    "    #count the word_count\n",
    "    txt = df_result.loc[i, 'text']\n",
    "#    txt = translation_clean(txt)\n",
    "    if txt != \"Actually image\":\n",
    "        word_count = len(txt.split())\n",
    "    else:\n",
    "        word_count = -1\n",
    "    df_result.loc[i,'word_count'] = word_count\n",
    "\n",
    "    try:\n",
    "        # print('trytrytrytrytrytrytrytry')\n",
    "        # print('df_result.loc[i,text]:')\n",
    "        # print(type(df_result.loc[i,'text']))\n",
    "        lang = detect(df_result.loc[i, 'text'])\n",
    "        if lang == 'en':\n",
    "            # print('ifififififififif')\n",
    "            df_result.loc[i, 'language'] = 'English'\n",
    "            print(str(i), df_result.corp_id[i], file_name, 'English')\n",
    "        else:\n",
    "            df_result.loc[i, 'language'] = lang\n",
    "            # print('elselseelseelseelse')\n",
    "            # if len(df_result.loc[i, 'text']) >= unit_limit:\n",
    "            #     # raw_str = temp_str\n",
    "            #     df_result.loc[i, 'translation'] = 'Too big'\n",
    "            #     print(str(i), df_result.corp_id[i], file_name, 'too big to translate')\n",
    "            # else:\n",
    "            #     try:\n",
    "            #         if len(df_result.loc[i, 'text']) <= 4000:\n",
    "            #             df_result.loc[i, 'translation'] = get_translated_text(df_result.loc[i, 'text'])\n",
    "            #         else:\n",
    "            #             lines = textwrap.wrap(df_result.loc[i, 'text'], 4000, break_long_words=False)\n",
    "            #             # translate_client = translate.Client()\n",
    "            #             df_result.loc[i, 'translation'] = ''.join([get_translated_text(x) for x in lines])\n",
    "            #         print(str(i), df_result.corp_id[i], file_name, 'Translated')\n",
    "            #     except Exception as e:\n",
    "            #         # raw_str = temp_str\n",
    "            #         df_result.loc[i, 'translation'] = 'Fail to translate'\n",
    "            #         print(str(i), df_result.corp_id[i], file_name, 'Fail to translate')\n",
    "            #         print(\"ERROR MESSAGE translation: \")\n",
    "            #         print(e)\n",
    "    except Exception as e:\n",
    "        print('ERROR MESSAGE detect:')\n",
    "        print(e)\n",
    "        df_result.loc[i, 'text'] = 'No content'\n",
    "        df_result.loc[i, 'page_num'] = np.NaN\n",
    "        df_result.loc[i, 'language'] = 'No content'\n",
    "        print(str(i), df_result.corp_id[i], file_name, 'No content')\n",
    "        \n",
    "    time.sleep(1)\n",
    "        \n",
    "#    ttable.append(df_result.iloc[[i]])\n",
    "    return df_result.iloc[[i]]\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#     writer = pd.ExcelWriter(fnx)\n",
    "#     df_result.loc[:, :].to_excel(writer, 'sheet1', header=True, index=True)\n",
    "#     writer.save()\n",
    "#     writer.close()\n",
    "#     df_result.loc[:, :].to_csv(fnc, index=True)\n",
    "#     print(str(i), df_result.corp_id[i], ' finish step', fnc)\n",
    "\n",
    "#     print(\"Finish textracting \" + df_result.corp_id[i] + \" !\")\n",
    "\n",
    "\n",
    "\n",
    "# This is to check if we failed to translate any text in df_result.\n",
    "# If there is, we try to translate those text agin\n",
    "'''\n",
    "reTrans(df_result)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['464159', 'COP_Chauveau_2017.pdf', 16948813]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_result = range(len(result_list))\n",
    "start=time.time()\n",
    "num_cores=multiprocessing.cpu_count()\n",
    "final_pd = Parallel(n_jobs=num_cores)(delayed(extract_text)(k) for k in tqdm.tqdm_notebook(list_result))\n",
    "\n",
    "print('------time used: {} ------------'.format(time.time()-start))\n",
    "\n",
    "for i in final_pd:\n",
    "    big_table=pd.concat([big_table,i],axis=0,sort=False)\n",
    "\n",
    "writer = pd.ExcelWriter(fnx)\n",
    "big_table.loc[:, :].to_excel(writer, 'sheet1', header=True, index=True)\n",
    "writer.save()\n",
    "writer.close()\n",
    "big_table.loc[:, :].to_csv(fnc, index=True)\n",
    "\n",
    "spreads = pd.read_csv(load_path + '/output44/2018_COP_phase_1_result.csv')\n",
    "spreads1 = spreads.loc[spreads['language'] == 'English']\n",
    "fnd=load_path + '/output44/2018_COP_phase_1_result_English.xlsx'\n",
    "fnf=load_path + '/output44/2018_COP_phase_1_result_English.csv'\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter(fnd)\n",
    "spreads1.loc[:, :].to_excel(writer, 'sheet1', header=True, index=True)\n",
    "writer.save()\n",
    "writer.close()\n",
    "spreads1.loc[:, :].to_csv(fnf, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in final_pd:\n",
    "    big_table=pd.concat([big_table,i],axis=0,sort=False)\n",
    "\n",
    "writer = pd.ExcelWriter(fnx)\n",
    "big_table.loc[:, :].to_excel(writer, 'sheet1', header=True, index=True)\n",
    "writer.save()\n",
    "writer.close()\n",
    "big_table.loc[:, :].to_csv(fnc, index=True)\n",
    "\n",
    "spreads = pd.read_csv(load_path + '/output2/2018_COP_phase_1_result.csv')\n",
    "spreads1 = spreads.loc[spreads['language'] == 'English']\n",
    "fnd=load_path + '/output2/2018_COP_phase_1_result_English.xlsx'\n",
    "fnf=load_path + '/output2/2018_COP_phase_1_result_English.csv'\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter(fnd)\n",
    "spreads1.loc[:, :].to_excel(writer, 'sheet1', header=True, index=True)\n",
    "writer.save()\n",
    "writer.close()\n",
    "spreads1.loc[:, :].to_csv(fnf, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in final_pd:\n",
    "    big_table=pd.concat([big_table,i],axis=0,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(fnx)\n",
    "big_table.loc[:, :].to_excel(writer, 'sheet1', header=True, index=True)\n",
    "writer.save()\n",
    "writer.close()\n",
    "big_table.loc[:, :].to_csv(fnc, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads = pd.read_csv(load_path + '/output2/2018_COP_phase_1_result.csv')\n",
    "spreads1 = spreads.loc[spreads['language'] == 'English']\n",
    "fnd=load_path + '/output2/2018_COP_phase_1_result_English.xlsx'\n",
    "fnf=load_path + '/output2/2018_COP_phase_1_result_English.csv'\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter(fnd)\n",
    "spreads1.loc[:, :].to_excel(writer, 'sheet1', header=True, index=True)\n",
    "writer.save()\n",
    "writer.close()\n",
    "spreads1.loc[:, :].to_csv(fnf, index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
